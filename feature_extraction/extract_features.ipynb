{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Clone repositories"
      ],
      "metadata": {
        "id": "l8c3pFSkuUqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/camilalaranjeira/seeing_without_looking.git\n",
        "!git clone https://github.com/WongKinYiu/yolov7.git seeing_without_looking/feature_extraction/"
      ],
      "metadata": {
        "id": "rW8eIwz7uJTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "It may take a while."
      ],
      "metadata": {
        "id": "5vOYYD7nuWwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop facenet-pytorch piq opennsfw2 tensorflow==2.11"
      ],
      "metadata": {
        "id": "xTY7nLPMuLe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download models"
      ],
      "metadata": {
        "id": "jh-bfQdFuYw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "%cd /content/seeing_without_looking/feature_extraction\n",
        "\n",
        "if not os.path.isfile('yolov7/yolov7.pt'):\n",
        "    !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt -P yolov7/\n",
        "\n",
        "if not os.path.isfile('scenes/whole_wideresnet18_places365.pth.tar'):\n",
        "    !gdown 1D6bGoJHuzXJhnr5KI70Zj1PkbGWWtWij -O scenes/ # whole_wideresnet18_places365.pth.tar\n",
        "\n",
        "if not os.path.isfile('nsfw_model/nsfw_mobilenet2.224x224.h5'):\n",
        "    !gdown 1t8cAnS8rNBQU8vo16CDAiBL0RuTJdesi -O nsfw_model/ #nsfw_mobilenet2.224x224.h5\n",
        "\n",
        "if not os.path.isdir('fitzpatrick'):\n",
        "    os.mkdir('fitzpatrick')\n",
        "    !gdown 1AEtQ2s4k5R7IKdrK6vs_zqH_DvXIeFUK -O fitzpatrick/ # shape_predictor_68_face_landmarks.dat\n",
        "\n",
        "if not os.path.isfile('model_age/vgg16_agegender.hdf5'):\n",
        "    !gdown 1ZF33ousEHhAwK8MmNXpuwmvVtXilVAJ_ -O model_age/\n",
        "\n",
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC0gMxzLuPGh",
        "outputId": "e50d89d7-9a17-4fe9-f734-78c34bc99556"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/seeing_without_looking/feature_extraction\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZF33ousEHhAwK8MmNXpuwmvVtXilVAJ_\n",
            "From (redirected): https://drive.google.com/uc?id=1ZF33ousEHhAwK8MmNXpuwmvVtXilVAJ_&confirm=t&uuid=4a48f442-1b29-41c8-b7da-59e9cd31f319\n",
            "To: /content/seeing_without_looking/feature_extraction/model_age/vgg16_agegender.hdf5\n",
            "100% 521M/521M [00:07<00:00, 70.3MB/s]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "QSTYAMhjyWWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isdir('images'):\n",
        "    !gdown 1yxBrC9d6Hfun9sxgePSr5hlzQ8ErNGbE #SOD images\n",
        "    !unzip -q images.zip\n",
        "    !rm images.zip"
      ],
      "metadata": {
        "id": "NbC4y9GWyYOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6fa6b2-fd2f-4fa9-de81-12001c155690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yxBrC9d6Hfun9sxgePSr5hlzQ8ErNGbE\n",
            "From (redirected): https://drive.google.com/uc?id=1yxBrC9d6Hfun9sxgePSr5hlzQ8ErNGbE&confirm=t&uuid=35c8290f-d6d4-4ef7-a558-b108a1b8397d\n",
            "To: /content/images.zip\n",
            "100% 1.00G/1.00G [00:11<00:00, 88.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inferences\n",
        "\n",
        "Imports."
      ],
      "metadata": {
        "id": "QygR3qP5y8dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import pandas as pd\n",
        "\n",
        "args = {\n",
        "    'device': '0',\n",
        "    'data_source': 'images', # path to dataset\n",
        "    'rootpath': '/content/seeing_without_looking/feature_extraction/' # path to third party methods\n",
        "}\n",
        "\n",
        "############## Objects ##############\n",
        "# https://github.com/WongKinYiu/yolov7\n",
        "sys.path.append(os.path.join(args['rootpath'], 'yolov7'))\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import check_img_size, non_max_suppression, scale_coords\n",
        "from utils.datasets import LoadImages\n",
        "from utils.torch_utils import select_device, TracedModel\n",
        "import torch\n",
        "import shutil\n",
        "#####################################\n",
        "\n",
        "############## Pornography ##############\n",
        "# Yahoo OpenNSFW\n",
        "import opennsfw2 as n2\n",
        "\n",
        "# NSFW-JS\n",
        "sys.path.append(args['rootpath'])\n",
        "from nsfw_model.nsfw_detector import predict\n",
        "#####################################\n",
        "\n",
        "######### DEMOGRAPHICS #########\n",
        "from skimage import io, transform\n",
        "import cv2, gc, time\n",
        "\n",
        "sys.path.append(os.path.join(args['rootpath'], 'model_age'))\n",
        "from faces import get_faces_mtcnn\n",
        "from configcnn import ConfigCNN\n",
        "from keras.models import model_from_json\n",
        "from fitzpatrick import Segmentation, SkinTone\n",
        "from tensorflow.python.keras import backend as K\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#####################################\n",
        "\n",
        "######### SCENES #########\n",
        "# https://github.com/CSAILVision/places365/blob/master/demo_pytorch_CAM.py\n",
        "sys.path.append(args['rootpath'])\n",
        "from scenes import places_torch as places\n",
        "from torch.autograd import Variable as V\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "#####################################\n",
        "\n",
        "######### QUALITY #########\n",
        "from piq import brisque\n",
        "from torchvision import transforms\n",
        "#####################################"
      ],
      "metadata": {
        "id": "8PKQQvaQv8bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Store filenames into final structure"
      ],
      "metadata": {
        "id": "V-M0BPEVucg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = 'images'\n",
        "samples = {'filenames': []}\n",
        "for filename in os.listdir(datapath):\n",
        "    ext = filename[filename.rfind('.')+1:]\n",
        "    if ext in ['jpg', 'png', 'jpeg', 'gif', 'tiff', 'bmp', 'webp']:\n",
        "        samples['filenames'].append(filename)\n",
        "\n",
        "filename_idx = {filename: idx for idx, filename in enumerate(samples['filenames'])}"
      ],
      "metadata": {
        "id": "emIu4rs2ucTs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Object detection\n",
        "From github repository of YOLOv7. Produces 3 novel columns:\n",
        "* object_name\n",
        "* object_bbox\n",
        "* object_conf"
      ],
      "metadata": {
        "id": "OzUMq2pDv2YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rootpath = args['rootpath']\n",
        "if not os.path.isdir(rootpath+'objects'):\n",
        "    os.mkdir(rootpath+'objects')\n",
        "    !gdown 1SAUaYzNJdQeZ2r1gmf6tLFB6oRclB4RA\n",
        "    shutil.move('coco_categories.csv', rootpath+'objects/coco_categories.csv')\n",
        "\n",
        "coco_categories = pd.read_csv(rootpath+'objects/coco_categories.csv')\n",
        "args['weights'] = 'yolov7/yolov7.pt'\n",
        "args['conf_thres'] = 0.25\n",
        "args['iou_thres'] =  0.5\n",
        "args['img_size'] =  640\n",
        "\n",
        "device = select_device(args['device'])\n",
        "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "imgsz = args['img_size']\n",
        "\n",
        "# Load model\n",
        "model = attempt_load(args['weights'], map_location=device)  # load FP32 model\n",
        "stride = int(model.stride.max())  # model stride\n",
        "imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "\n",
        "model = TracedModel(model, device, imgsz)\n",
        "model.half()\n",
        "\n",
        "dataset = LoadImages(args['data_source'], img_size=imgsz, stride=stride)\n",
        "names = model.module.names if hasattr(model, 'module') else model.names\n",
        "\n",
        "# Run inference\n",
        "if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "old_img_w = old_img_h = imgsz\n",
        "old_img_b = 1\n",
        "\n",
        "###########################\n",
        "samples['object_name'] = [None]*len(samples['filenames'])\n",
        "samples['object_bbox'] = [None]*len(samples['filenames'])\n",
        "samples['object_conf'] = [None]*len(samples['filenames'])\n",
        "###########################\n",
        "\n",
        "for en, (path, img, im0s, vid_cap) in enumerate(dataset):\n",
        "    if en % 100 == 0:\n",
        "        print(f'\\r{en}', flush=True, end='')\n",
        "\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "\n",
        "    # Warmup\n",
        "    if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
        "        old_img_b = img.shape[0]\n",
        "        old_img_h = img.shape[2]\n",
        "        old_img_w = img.shape[3]\n",
        "        for i in range(3):\n",
        "            model(img, augment=False)[0]\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
        "        pred = model(img, augment=False)[0]\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred, args['conf_thres'], args['iou_thres'])\n",
        "\n",
        "    for i, det in enumerate(pred):  # detections per image\n",
        "        if len(det):\n",
        "            # Rescale boxes from img_size to im0 size\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n",
        "            det[:, :4:2]  /= im0s.shape[0] # normalize\n",
        "            det[:, 1:4:2] /= im0s.shape[1] # normalize\n",
        "\n",
        "            xyxy    = det[:, :4].detach().cpu().tolist()\n",
        "            conf    = det[:, 4].detach().cpu().tolist()\n",
        "            classes = det[:, 5].detach().cpu().tolist()\n",
        "\n",
        "            class_names = []\n",
        "            for c in classes:\n",
        "                row = coco_categories.iloc[int(c)]\n",
        "                classname = '/'.join( (row['supercategory'], row['category']) )\n",
        "                class_names.append(classname)\n",
        "\n",
        "            idx = filename_idx[os.path.basename(path)]\n",
        "            samples['object_name'][idx] = class_names\n",
        "            samples['object_bbox'][idx] = xyxy\n",
        "            samples['object_conf'][idx] = conf"
      ],
      "metadata": {
        "id": "7PTUkHEmviv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3776658e-c3bf-4dd4-ec7b-41bc3392888a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Convert model to Traced-model... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:836: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if param.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Pornography\n",
        "\n",
        "Two models:\n",
        "* Yahoo OpenNSFW: https://github.com/bhky/opennsfw2\n",
        "* NSFW-JS: https://github.com/infinitered/nsfwjs\n",
        "\n"
      ],
      "metadata": {
        "id": "vsYMKWX_zMBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples['porn'] = []\n",
        "\n",
        "for imgfile in samples['filenames']:\n",
        "    imgfile = os.path.join(args['data_source'], imgfile)\n",
        "    nsfw_probability = n2.predict_image(imgfile)\n",
        "    samples['porn'].append(nsfw_probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RukjR14bQx8C",
        "outputId": "4e57563d-00fc-4b3a-e52c-d2861f70fcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Pre-trained weights will be downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/bhky/opennsfw2/releases/download/v0.1.0/open_nsfw_weights.h5\n",
            "To: /root/.opennsfw2/weights/open_nsfw_weights.h5\n",
            "100%|██████████| 24.2M/24.2M [00:00<00:00, 254MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.999843955039978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rootpath = args['rootpath']\n",
        "model = predict.load_model(f'{rootpath}/nsfw_model/nsfw_mobilenet2.224x224.h5')\n",
        "\n",
        "samples['porn_2019'] = []\n",
        "for k, filename in enumerate(samples['filenames']):\n",
        "    filename = os.path.join(args['data_source'], filename)\n",
        "    output = predict.classify(model, filename)\n",
        "    output = list(output.values())[0]\n",
        "\n",
        "    samples['porn_2019'].append(\n",
        "        [output['neutral'], output['drawings'],\n",
        "         output['hentai'], output['sexy'],\n",
        "         output['porn']]\n",
        "    )"
      ],
      "metadata": {
        "id": "l6isY82RzMNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97edbe8c-2886-49db-d149-f8c6be88ec8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "{'drawings': 0.0001275123649975285, 'hentai': 0.0366964191198349, 'neutral': 0.0009582224884070456, 'porn': 0.7896934747695923, 'sexy': 0.17252439260482788}\n",
            "\n",
            "NSFW JS running time: 1.2891\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Scenes"
      ],
      "metadata": {
        "id": "37_0MPwB7NPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rootpath = os.path.join(args['rootpath'], 'scenes')\n",
        "\n",
        "# load the labels\n",
        "classes, classes_macro = places.load_labels(rootpath)\n",
        "labels_IO = []\n",
        "for key in sorted(classes_macro.keys()):\n",
        "    labels_IO.append(classes_macro[key][0])\n",
        "labels_IO = np.array(labels_IO)\n",
        "\n",
        "# load the model\n",
        "model = places.load_model(rootpath)\n",
        "model.avgpool = nn.AvgPool2d(14)\n",
        "\n",
        "# load the transformer\n",
        "transf = places.returnTF() # image transformer\n",
        "\n",
        "# get the softmax weight\n",
        "params = list(model.parameters())\n",
        "weight_softmax = params[-2].data.numpy()\n",
        "\n",
        "samples['scene'] = []\n",
        "samples['scene_conf'] = []\n",
        "samples['scene_io'] = []\n",
        "\n",
        "for imgfile in samples['filenames']:\n",
        "    imgfile = os.path.join(args['data_source'], imgfile)\n",
        "    img = places.imreadRotate(imgfile)\n",
        "    with torch.no_grad():\n",
        "        input_img = V(transf(img).unsqueeze(0))\n",
        "\n",
        "        # forward pass\n",
        "        logit = model.forward(input_img)\n",
        "        h_x = F.softmax(logit).data.squeeze()\n",
        "        probs, idx = h_x.sort(0, True)\n",
        "\n",
        "    # < 0.5: indoor\n",
        "    io_image = np.mean(labels_IO[idx[:10].numpy()]) # vote for the indoor or outdoor\n",
        "    samples['scene_io'].append(io_image) # 0: indoor, 1: outdoor\n",
        "\n",
        "    out = classes[idx[0]] # topcategory\n",
        "    prob = probs[0] # conf\n",
        "\n",
        "    macro = classes_macro[out]\n",
        "    macro_lst = []\n",
        "    if macro[0] == 0: # INDOOR\n",
        "        macro_lst.append('indoor')\n",
        "        macro_lst.append('residential' if macro[1]==0 else 'commercial')\n",
        "    else:\n",
        "        macro_lst.append('outdoor')\n",
        "        macro_lst.append('nature' if macro[1]==0 else 'urban')\n",
        "\n",
        "    macro_lst.append(out)\n",
        "    samples['scene'].append('/'.join(macro_lst))\n",
        "    samples['scene_conf'].append(prob.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHHDNNlB7Mft",
        "outputId": "7cfdd947-89cb-4628-b172-e89adf63a468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'wideresnet.ResNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dont rotate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-af3e635915ba>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  h_x = F.softmax(logit).data.squeeze()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demographics"
      ],
      "metadata": {
        "id": "Oe3OWrXZ9JrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## model for age, child and gender\n",
        "model_age  = model_from_json(\n",
        "    open(os.path.join(args['rootpath'], 'model_age', 'vgg16_agegender_model.json')).read()\n",
        ")\n",
        "device = args['device']\n",
        "\n",
        "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
        "sess = tf.Session(config=config)\n",
        "K.set_session(sess)\n",
        "\n",
        "## model for segmentation + ITA calculation\n",
        "skin_tone = SkinTone(modelpath=os.path.join(\n",
        "                    args['rootpath'], 'fitzpatrick',\n",
        "                    'shape_predictor_68_face_landmarks.dat'))\n",
        "\n",
        "attributes = ['face_name', 'face_bbox', 'face_conf',\n",
        "             'age', 'child', 'gender', 'skin_ita']\n",
        "for attr in attributes:\n",
        "    samples[attr] = []\n",
        "\n",
        "with sess:\n",
        "    model_age.load_weights(os.path.join(args['rootpath'], 'model_age', 'vgg16_agegender.hdf5'))\n",
        "\n",
        "    for k, filename in enumerate(samples['filenames']):\n",
        "        if k % 100 == 0:\n",
        "            print(f'\\rExtracting signals from MTCNN + Skin + Age + Child + Gender: {k}/{len(samples)}', end='', flush=True)\n",
        "\n",
        "        imgpath = os.path.join(args['data_source'], filename)\n",
        "\n",
        "        results = get_faces_mtcnn(imgpath, device)\n",
        "        if len(results) == 0:\n",
        "            for attr in attributes:\n",
        "                samples[attr].append(np.nan)\n",
        "            continue\n",
        "\n",
        "        samples['face_name'].append('has_face')\n",
        "        samples['face_bbox'].append([res[1] for res in results])\n",
        "        samples['face_conf'].append([res[2] for res in results])\n",
        "\n",
        "        age    = []\n",
        "        child  = []\n",
        "        gender = []\n",
        "        skin_ita = []\n",
        "\n",
        "        for res in results:\n",
        "            ita, patch = skin_tone.ITA(res[0])\n",
        "            skin_ita.append(ita)\n",
        "\n",
        "            face = transform.resize(res[0], (128, 128))\n",
        "            predictions = model_age.predict(face[None,:,:,:])\n",
        "\n",
        "            age.append(predictions[0][0].tolist())\n",
        "            child.append(predictions[1][0][0].item())\n",
        "            gender.append(predictions[2][0][0].item())\n",
        "\n",
        "        samples['age'].append(age)\n",
        "        samples['child'].append(child)\n",
        "        samples['gender'].append(gender)\n",
        "        samples['skin_ita'].append(skin_ita)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMdGXCvv9JhD",
        "outputId": "af9c3827-7b75-4fe1-c7e9-3e57d4c2fbb9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rExtracting signals from MTCNN + Skin + Age + Child + Gender: 0/8"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FACE running time: 0.5246\n",
            "\n"
          ]
        }
      ]
    }
  ]
}